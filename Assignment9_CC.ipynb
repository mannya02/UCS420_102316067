{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e68453a-f333-4ff5-8e81-746528d8eb67",
   "metadata": {},
   "source": [
    "# ASSIGNMENT 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f0f72b-13bb-4979-b4b2-bb7cd8be3f7b",
   "metadata": {},
   "source": [
    "### Q1. Write a unique paragraph (5-6 sentences) about your favorite topic (e.g., sports, technology, food, books, etc.).\n",
    "#### 1. Convert text to lowercase and remove punctuation.\n",
    "#### 2. Tokenize the text into words and sentences.\n",
    "#### 3. Remove stopwords (using NLTK's stopwords list).\n",
    "#### 4. Display word frequency distribution (excluding stopwords)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65aa8645-4e0e-4c68-9ef9-3005ed5a91a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentences:\n",
      " ['Agentic AI is an exciting advancement in artificial intelligence where systems can act independently.', 'These AI systems are capable of making decisions, learning from experiences, and adapting to their environment.', 'They function with minimal human intervention and have great potential in various fields such as healthcare, \\neducation, and robotics.', 'As AI continues to evolve, agentic systems could become companions and collaborators.', 'The rise of Agentic AI also raises important questions about responsibility, safety, and ethics.'] \n",
      "\n",
      "Filtered Words (no stopwords):\n",
      " ['agentic', 'ai', 'exciting', 'advancement', 'artificial', 'intelligence', 'systems', 'act', 'independently', 'ai', 'systems', 'capable', 'making', 'decisions', 'learning', 'experiences', 'adapting', 'environment', 'function', 'minimal', 'human', 'intervention', 'great', 'potential', 'various', 'fields', 'healthcare', 'education', 'robotics', 'ai', 'continues', 'evolve', 'agentic', 'systems', 'could', 'become', 'companions', 'collaborators', 'rise', 'agentic', 'ai', 'also', 'raises', 'important', 'questions', 'responsibility', 'safety', 'ethics'] \n",
      "\n",
      "Top Word Frequencies:\n",
      "ai: 4\n",
      "agentic: 3\n",
      "systems: 3\n",
      "exciting: 1\n",
      "advancement: 1\n",
      "artificial: 1\n",
      "intelligence: 1\n",
      "act: 1\n",
      "independently: 1\n",
      "capable: 1\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import FreqDist\n",
    "\n",
    "paragraph = \"\"\"Agentic AI is an exciting advancement in artificial intelligence where systems can act independently. \n",
    "These AI systems are capable of making decisions, learning from experiences, and adapting to their environment. \n",
    "They function with minimal human intervention and have great potential in various fields such as healthcare, \n",
    "education, and robotics. As AI continues to evolve, agentic systems could become companions and collaborators. \n",
    "The rise of Agentic AI also raises important questions about responsibility, safety, and ethics.\"\"\"\n",
    "\n",
    "text_cleaned = paragraph.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "words = word_tokenize(text_cleaned)\n",
    "sentences = sent_tokenize(paragraph)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words if word not in stop_words and word.isalpha()]\n",
    "\n",
    "fdist = FreqDist(filtered_words)\n",
    "\n",
    "print(\"Original Sentences:\\n\", sentences, \"\\n\")\n",
    "print(\"Filtered Words (no stopwords):\\n\", filtered_words, \"\\n\")\n",
    "print(\"Top Word Frequencies:\")\n",
    "for word, freq in fdist.most_common(10):\n",
    "    print(f\"{word}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0b6451-320e-4979-abb7-e3dd81bdae77",
   "metadata": {},
   "source": [
    "### Q2. Stemming and Lemmatization\n",
    "#### 1. Take the tokenized words from Question 1 (after stopword removal).\n",
    "#### 2. Apply stemming using NLTK's PorterStemmer and LancasterStemmer.\n",
    "#### 3. Apply lemmatization using NLTK's WordNetLemmatizer.\n",
    "#### 4. Compare and display results of both techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "765284dd-385d-49cb-b59e-3e110e1caafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word | PorterStem | LancasterStem | Lemmatized\n",
      "\n",
      "agentic      | agent        | ag             | agentic\n",
      "ai           | ai           | ai             | ai\n",
      "exciting     | excit        | excit          | exciting\n",
      "advancement  | advanc       | adv            | advancement\n",
      "artificial   | artifici     | art            | artificial\n",
      "intelligence | intellig     | intellig       | intelligence\n",
      "systems      | system       | system         | system\n",
      "act          | act          | act            | act\n",
      "independently | independ     | independ       | independently\n",
      "ai           | ai           | ai             | ai\n",
      "systems      | system       | system         | system\n",
      "capable      | capabl       | cap            | capable\n",
      "making       | make         | mak            | making\n",
      "decisions    | decis        | decid          | decision\n",
      "learning     | learn        | learn          | learning\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"Word | PorterStem | LancasterStem | Lemmatized\\n\")\n",
    "for word in filtered_words[:15]:  \n",
    "    print(f\"{word:<12} | {porter.stem(word):<12} | {lancaster.stem(word):<14} | {lemmatizer.lemmatize(word)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d4b15b-2a03-4bd2-9284-1b8a150b27b6",
   "metadata": {},
   "source": [
    "### Q3. Regular Expressions and Text Splitting\n",
    "#### 1. Take their original text from Question 1.\n",
    "#### 2. Use regular expressions to:\n",
    "#### a. Extract all words with more than 5 letters.\n",
    "#### b. Extract all numbers (if any exist in their text).\n",
    "#### c. Extract all capitalized words.\n",
    "#### 3. Use text splitting techniques to:\n",
    "#### a. Split the text into words containing only alphabets (removing digits and special characters).\n",
    "#### b. Extract words starting with a vowel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6de68da6-effa-45e1-bff2-933c16b760b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words with >5 letters: ['Agentic', 'exciting', 'advancement', 'artificial', 'intelligence', 'systems', 'independently', 'systems', 'capable', 'making', 'decisions', 'learning', 'experiences', 'adapting', 'environment', 'function', 'minimal', 'intervention', 'potential', 'various', 'fields', 'healthcare', 'education', 'robotics', 'continues', 'evolve', 'agentic', 'systems', 'become', 'companions', 'collaborators', 'Agentic', 'raises', 'important', 'questions', 'responsibility', 'safety', 'ethics']\n",
      "Numbers: []\n",
      "Capitalized Words: ['Agentic', 'These', 'They', 'As', 'The', 'Agentic']\n",
      "Alphabetic Words Only: ['Agentic', 'AI', 'is', 'an', 'exciting', 'advancement', 'in', 'artificial', 'intelligence', 'where']\n",
      "Words starting with vowels: ['Agentic', 'AI', 'is', 'an', 'exciting', 'advancement', 'in', 'artificial', 'intelligence', 'act', 'independently', 'AI', 'are', 'of', 'experiences', 'and', 'adapting', 'environment', 'intervention', 'and', 'in', 'as', 'education', 'and', 'As', 'AI', 'evolve', 'agentic', 'and', 'of', 'Agentic', 'AI', 'also', 'important', 'about', 'and', 'ethics']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "words_5plus = re.findall(r'\\b\\w{6,}\\b', paragraph)\n",
    "\n",
    "numbers = re.findall(r'\\b\\d+\\b', paragraph)\n",
    "\n",
    "capitalized_words = re.findall(r'\\b[A-Z][a-z]+\\b', paragraph)\n",
    "\n",
    "alpha_words = re.findall(r'\\b[a-zA-Z]+\\b', paragraph)\n",
    "\n",
    "vowel_words = re.findall(r'\\b[aeiouAEIOU][a-zA-Z]*\\b', paragraph)\n",
    "\n",
    "print(\"Words with >5 letters:\", words_5plus)\n",
    "print(\"Numbers:\", numbers)\n",
    "print(\"Capitalized Words:\", capitalized_words)\n",
    "print(\"Alphabetic Words Only:\", alpha_words[:10])  # show first 10\n",
    "print(\"Words starting with vowels:\", vowel_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dc62f3-7235-43e6-8a8b-739c23d9db92",
   "metadata": {},
   "source": [
    "### Q4. Custom Tokenization & Regex-based Text Cleaning\n",
    "#### 1. Take original text from Question 1.\n",
    "#### 2. Write a custom tokenization function that:\n",
    "#### a. Removes punctuation and special symbols, but keeps contractions (e.g.,\"isn't\" should not be split into \"is\" and \"n't\").\n",
    "#### b. Handles hyphenated words as a single token (e.g., \"state-of-the-art\" remains a single token).\n",
    "#### c. Tokenizes numbers separately but keeps decimal numbers intact (e.g., \"3.14\" should remain as is).\n",
    "#### 3. Use Regex Substitutions (re.sub) to:\n",
    "#### a. Replace email addresses with '<EMAIL>' placeholder.\n",
    "#### b. Replace URLs with '<URL>' placeholder.\n",
    "#### c. Replace phone numbers (formats: 123-456-7890 or +91 9876543210) with '<PHONE>' placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b18f9953-39cb-4432-9e51-726fbb734ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Tokens: ['Contact', 'us', 'at', 'ai', 'example', 'com', 'or', 'visit', 'https', 'agentic', 'ai', 'Call', '123', '456', '7890', 'or', '91', '9876543210', 'This', 'state-of-the-art', 'model', 'isn', 't', 'bad', 'It', 'costs', '3.14', 'dollars']\n",
      "Cleaned Text: Contact us at <EMAIL> or visit <URL> Call <PHONE> or <PHONE>. This state-of-the-art model isn’t bad. It costs 3.14 dollars!\n"
     ]
    }
   ],
   "source": [
    "def custom_tokenizer(text):\n",
    "    pattern = r\"(?:\\d+\\.\\d+)|(?:[a-zA-Z]+(?:-[a-zA-Z]+)*)|(?:\\w+'\\w+)|(?:\\d+)\"\n",
    "    return re.findall(pattern, text)\n",
    "\n",
    "sample_text = \"Contact us at ai@example.com or visit https://agentic.ai. Call 123-456-7890 or +91 9876543210. This state-of-the-art model isn’t bad. It costs 3.14 dollars!\"\n",
    "\n",
    "tokens = custom_tokenizer(sample_text)\n",
    "\n",
    "text_cleaned = re.sub(r'\\S+@\\S+', '<EMAIL>', sample_text)\n",
    "text_cleaned = re.sub(r'https?://\\S+', '<URL>', text_cleaned)\n",
    "text_cleaned = re.sub(r'(\\+91\\s?\\d{10}|\\d{3}[-.\\s]?\\d{3}[-.\\s]?\\d{4})', '<PHONE>', text_cleaned)\n",
    "\n",
    "print(\"Custom Tokens:\", tokens)\n",
    "print(\"Cleaned Text:\", text_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1728cff-8776-4f0a-a614-f65826253831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4ff0a7-737b-4e7a-b438-b9fd22b52429",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
